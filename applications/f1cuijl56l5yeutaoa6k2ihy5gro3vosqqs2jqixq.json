{
  "Version": "1.3",
  "ID": "f1cuijl56l5yeutaoa6k2ihy5gro3vosqqs2jqixq",
  "Issue Number": "175",
  "Client": {
    "Name": "laion",
    "Region": "Afghanistan",
    "Industry": "Life Science / Healthcare",
    "Website": "https://laion.ai/blog/laion-5b/\nhttps://github.com/LAION-AI/laion-3d\nhttps://huggingface.co/datasets/laion/laion-high-resolution",
    "Social Media": "https://laion.ai/blog/laion-5b/\nhttps://laion.ai/blog/laion-400-open-dataset/\nhttps://laion.ai/blog/large-openclip/\nhttps://laion.ai/blog/laion-aesthetics/\nhttps://github.com/LAION-AI/laion-3d\nhttps://huggingface.co/datasets/laion/laion-high-resolution",
    "Social Media Type": "Slack",
    "Role": "Data Preparer"
  },
  "Project": {
    "Brief history of your project and organization": "LAION, as a non-profit organization, provides datasets, tools and models to liberate machine learning research. By doing so, we encourage open public education and a more environment-friendly use of resources by reusing existing datasets and models.\n\nWe present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our NeurIPS2022 paper",
    "Is this project associated with other projects/ecosystem stakeholders?": "No",
    "Describe the data being stored onto Filecoin": "Laion was initiated with the Laion5B project that successfully produced a 5B (image, text) pairs dataset by processing commoncrawl and filtering with clip. That method proved that itâ€™s cheap to collect large scale dataset from the web using models like clip that give the similarity between items from 2 modalities.\n\nMany models have been trained on laion400m proving the value of this method, with in particular openclip that reproduced the same results that the initial openai clip.\n\n",
    "Where was the data currently stored in this dataset sourced from": "AWS Cloud",
    "How do you plan to prepare the dataset": "",
    "Please share a sample of the data (a link to a file, an image, a table, etc., are good ways to do this.)": "[30k samples from thingiverse](https://zenodo.org/record/1098527) (3d printing STL model files)\n[Fusion360Gallery](https://github.com/AutodeskAILab/Fusion360GalleryDataset)\n[Amazon Berkeley Objects (ABO)](https://amazon-berkeley-objects.s3.amazonaws.com/index.html#download), a dataset of Amazon products with metadata, catalog images, and 3D models.\n[Large Geometric Models Archive](https://www.cc.gatech.edu/projects/large_models/about.html)\n[FaceScape](https://facescape.nju.edu.cn/Page_Download/), a large-scale detailed 3D face dataset (application required).\n[Redwood 3DScan](https://github.com/isl-org/redwood-3dscan), more than ten thousand 3D scans of real objects.\n[Human3.6M](http://vision.imar.ro/human3.6m/description.php), 3.6 million 3D human poses and corresponding images.\n[Semantic3D](https://www.semantic3d.net/), a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total.\n[SceneNN / ObjectNN](https://github.com/hkust-vgd/scenenn), an RGB-D dataset with more than 100 indoor scenes along with RGB-D objects extracted and split into 20 categories.\n[3D-FRONT](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset), a dataset of 3D furnished rooms with layouts and semantics.\n[3D-FUTURE](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future), a dataset of 3D furniture shapes with textures.\n[ABC](https://deep-geometry.github.io/abc-dataset/0), a collection of one million Computer-Aided Design (CAD) models\n[Structured3D](https://structured3d-dataset.org/#download), a large-scale photo-realistic dataset containing 3.5K house designs with a variety of ground truth 3D structure annotations.\n[ShapeNet](https://shapenet.org/), a richly-annotated, large-scale dataset of 3D shapes.\n[FixIt!](https://drive.google.com/drive/folders/1h9kMRilQcjbD4Tyt58pmMUEnMIicNATi), a dataset that contains about 5k poorly-designed 3D physical objects paired with choices to fix them.\n[ModelNet](http://modelnet.cs.princeton.edu/#), a comprehensive clean collection of 3D CAD models for objects.",
    "Confirm that this is a public dataset that can be retrieved by anyone on the network (i.e., no specific permissions or access rights are required to view the data)": "[x] I confirm",
    "What is the expected retrieval frequency for this data": "Yearly",
    "For how long do you plan to keep this dataset stored on Filecoin": "2 to 3 years",
    "In which geographies do you plan on making storage deals": "Greater China, Asia other than Greater China, North America, Europe",
    "How will you be distributing your data to storage providers": "Cloud storage (i.e. S3), HTTP or FTP server, Shipping hard drives",
    "Please list the provider IDs and location of the storage providers you will be working with. Note that it is a requirement to list a minimum of 5 unique provider IDs, and that your client address will be verified against this list in the future": "f03601451 Hong Kong\nf03609158 Hong Kong\nf03619150 shenzhen\nf02826762 Hong Kong\nf02827135 Hong Kong\nf02827010 Hong Kong\nf02825281 Xinjiang\nf03619143 HongKong\n ",
    "Can you confirm that you will follow the Fil+ guideline (Data owner should engage at least 4 SPs and no single SP ID should receive >30% of a client's allocated DataCap)": "Yes"
  },
  "Datacap": {
    "Type": "ldn-v3",
    "Data Type": "Slingshot",
    "Total Requested Amount": "8PiB",
    "Single Size Dataset": "1PiB",
    "Replicas": 8,
    "Weekly Allocation": "1.5PiB"
  },
  "Lifecycle": {
    "State": "Submitted",
    "Validated At": "",
    "Validated By": "",
    "Active": true,
    "Updated At": "2025-05-31 14:13:31.935819137 UTC",
    "Active Request ID": "",
    "On Chain Address": "f1cuijl56l5yeutaoa6k2ihy5gro3vosqqs2jqixq",
    "Multisig Address": "false",
    "edited": false
  },
  "Allocation Requests": []
}